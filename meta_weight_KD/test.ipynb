{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0002467198171342,\n",
       " 0.0009866357858642205,\n",
       " 0.002219017698460002,\n",
       " 0.0039426493427610065,\n",
       " 0.006155829702431115,\n",
       " 0.008856374635655584,\n",
       " 0.012041619030626283,\n",
       " 0.015708419435684462,\n",
       " 0.019853157161528467,\n",
       " 0.02447174185242318,\n",
       " 0.029559615522887217,\n",
       " 0.035111757055874326,\n",
       " 0.041122687158009485,\n",
       " 0.04758647376699021,\n",
       " 0.054496737905816106,\n",
       " 0.061846659978068264,\n",
       " 0.06962898649802818,\n",
       " 0.07783603724899246,\n",
       " 0.08645971286271914]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class CosineDecay(object):\n",
    "    def __init__(self,\n",
    "                max_value,\n",
    "                min_value,\n",
    "                num_loops):\n",
    "        self._max_value = max_value\n",
    "        self._min_value = min_value\n",
    "        self._num_loops = num_loops\n",
    "\n",
    "    def get_value(self, i):\n",
    "        if i < 0:\n",
    "            i = 0\n",
    "        if i >= self._num_loops:\n",
    "            i = self._num_loops\n",
    "        value = (math.cos(i * math.pi / self._num_loops) + 1.0) * 0.5\n",
    "        value = value * (self._max_value - self._min_value) + self._min_value\n",
    "        return 1-value\n",
    "    \n",
    "gradient_decay = CosineDecay(max_value=1, min_value=0, num_loops=100)\n",
    "[gradient_decay.get_value(i) for i in range(20)]\n",
    "# 1 + 20 * torch.sigmoid(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<GradientReversalFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Global_T(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Global_T, self).__init__()\n",
    "        \n",
    "        self.global_T = nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "        self.grl = GradientReversal()\n",
    "\n",
    "    def forward(self, fake_input1, fake_input2, lambda_):\n",
    "        return self.grl(self.global_T, lambda_)\n",
    "\n",
    "\n",
    "from torch.autograd import Function\n",
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
    "    Forward pass is the identity function. In the backward pass,\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = lambda_ * grads\n",
    "        # print(dx)\n",
    "        return dx, None\n",
    "\n",
    "\n",
    "class GradientReversal(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GradientReversal, self).__init__()\n",
    "        # self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x, lambda_):\n",
    "        return GradientReversalFunction.apply(x, lambda_)\n",
    "    \n",
    "\n",
    "model = Global_T()\n",
    "input = torch.rand(24,24,24)\n",
    "input2 = torch.rand(24,24,24)\n",
    "\n",
    "out = model(input, input2, 2)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.util import MadKD\n",
    "\n",
    "m = torch.load(\"mlp.pt\")\n",
    "[print(m.mlp[i].weight) for i in [0, 2, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients before GRL: tensor([[ 0.1056, -0.3990, -0.5509, -0.5313,  0.6619, -0.8576,  0.0689, -0.2342,\n",
      "         -0.5758, -1.0041]])\n",
      "Gradients after GRL: tensor([[-0.1056,  0.3990,  0.5509,  0.5313, -0.6619,  0.8576, -0.0689,  0.2342,\n",
      "          0.5758,  1.0041]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    def __init__(self, weight=1.0):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "        self.weight = torch.tensor(weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.weight)\n",
    "\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        ctx.save_for_backward(weight)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        weight, = ctx.saved_tensors\n",
    "        grad_input = weight * grad_output.neg()\n",
    "        return grad_input, None\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "        self.grl = GradientReversalLayer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.grl(x)\n",
    "        return x\n",
    "    \n",
    "# 创建模型和优化器\n",
    "model = SimpleModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 输入数据\n",
    "input_data = torch.randn(1, 10, requires_grad=True)\n",
    "\n",
    "# 前向传播\n",
    "output_before_grl = model(input_data)\n",
    "loss_before_grl = output_before_grl.sum()\n",
    "\n",
    "# 反向传播\n",
    "optimizer.zero_grad()\n",
    "loss_before_grl.backward()\n",
    "print(\"Gradients before GRL:\", model.fc.weight.grad)\n",
    "\n",
    "\n",
    "# 前向传播（梯度翻转层不改变前向传播的结果）\n",
    "input_data_grl = model(input_data)\n",
    "# 添加梯度翻转层\n",
    "grl_layer = GradientReversalLayer()\n",
    "output_after_grl = grl_layer(input_data_grl)\n",
    "loss_after_grl = output_after_grl.sum()\n",
    "\n",
    "# 反向传播\n",
    "optimizer.zero_grad()\n",
    "loss_after_grl.backward()\n",
    "print(\"Gradients after GRL:\", model.fc.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[4.5864e+36, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00],\n",
       "        [4.3664e+36, 0.0000e+00],\n",
       "        [4.3664e+36, 0.0000e+00],\n",
       "        [4.5864e+36, 0.0000e+00],\n",
       "        [4.5864e+36, 0.0000e+00],\n",
       "        [4.5864e+36, 0.0000e+00],\n",
       "        [4.5864e+36, 1.6489e+30],\n",
       "        [4.5864e+36, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00]], device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[69,  0],\n",
       "        [ 1,  0],\n",
       "        [27,  0],\n",
       "        [27,  0],\n",
       "        [69,  0],\n",
       "        [69,  0],\n",
       "        [69,  0],\n",
       "        [69, 27],\n",
       "        [69,  0],\n",
       "        [ 1,  0]], device='cuda:0'))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kd_loss(y_s, y_t, temperature):\n",
    "    p_s = F.log_softmax(y_s/temperature, dim=1)\n",
    "    p_t = F.softmax(y_t/temperature, dim=1)\n",
    "    loss = nn.KLDivLoss(reduction='none')(p_s, p_t) * (temperature**2)\n",
    "    return loss\n",
    "\n",
    "a = torch.load('y_s_model.pth')\n",
    "b = torch.load('y_t_model.pth')\n",
    "torch.set_printoptions(threshold=10000, linewidth=1000)\n",
    "loss = kd_loss(a, b, 4)\n",
    "loss, loss.sum(dim=1), loss.sum(dim=1).shape\n",
    "torch.topk(loss[0:10,:], k=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-4.9106e-02, -7.6506e-02,  5.0916e-02, -3.6265e-02,  7.5159e-02,\n",
       "           7.8946e-02, -2.7386e-04,  3.9537e-02],\n",
       "         [ 1.5538e-02,  2.4208e-02, -1.6111e-02,  1.1475e-02, -2.3782e-02,\n",
       "          -2.4980e-02,  8.6655e-05, -1.2510e-02],\n",
       "         [-9.3517e-03, -1.4570e-02,  9.6963e-03, -6.9063e-03,  1.4313e-02,\n",
       "           1.5034e-02, -5.2155e-05,  7.5294e-03],\n",
       "         [ 4.1071e-03,  6.3987e-03, -4.2584e-03,  3.0331e-03, -6.2861e-03,\n",
       "          -6.6028e-03,  2.2904e-05, -3.3068e-03],\n",
       "         [ 3.5342e-02,  5.5062e-02, -3.6645e-02,  2.6101e-02, -5.4093e-02,\n",
       "          -5.6818e-02,  1.9710e-04, -2.8455e-02],\n",
       "         [-1.3887e-02, -2.1636e-02,  1.4399e-02, -1.0256e-02,  2.1255e-02,\n",
       "           2.2326e-02, -7.7446e-05,  1.1181e-02],\n",
       "         [-1.5707e-02, -2.4471e-02,  1.6285e-02, -1.1600e-02,  2.4040e-02,\n",
       "           2.5251e-02, -8.7590e-05,  1.2646e-02],\n",
       "         [ 5.2004e-02,  8.1020e-02, -5.3920e-02,  3.8405e-02, -7.9594e-02,\n",
       "          -8.3604e-02,  2.9002e-04, -4.1870e-02],\n",
       "         [-4.6192e-02, -7.1965e-02,  4.7894e-02, -3.4113e-02,  7.0698e-02,\n",
       "           7.4260e-02, -2.5761e-04,  3.7191e-02],\n",
       "         [ 3.9742e-03,  6.1917e-03, -4.1207e-03,  2.9350e-03, -6.0827e-03,\n",
       "          -6.3892e-03,  2.2165e-05, -3.1998e-03]]),\n",
       " tensor([[-0.0520, -0.1062,  0.1497,  0.2289,  0.3246,  0.1030,  0.2530,  0.2410,\n",
       "          -0.0579,  0.0974],\n",
       "         [ 0.0520,  0.1062, -0.1497, -0.2289, -0.3246, -0.1030, -0.2530, -0.2410,\n",
       "           0.0579, -0.0974]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def kd_loss(y_s, y_t, temperature):\n",
    "    p_s = F.log_softmax(y_s/temperature, dim=1)\n",
    "    p_t = F.softmax(y_t/temperature, dim=1)\n",
    "    loss = nn.KLDivLoss(reduction='batchmean')(p_s, p_t) * (temperature**2)\n",
    "    return loss\n",
    "\n",
    "x = torch.randn(2, 8, requires_grad=True)\n",
    "gt = torch.ones((2), dtype=torch.long)\n",
    "\n",
    "ms = nn.Linear(8, 10)\n",
    "mt = nn.Linear(8, 10)\n",
    "\n",
    "m = nn.Linear(10, 2)\n",
    "\n",
    "opt = optim.SGD(nn.ModuleList([ms, m]).parameters(), lr=0.1)\n",
    "\n",
    "p = ms(x)  \n",
    "with torch.no_grad(): \n",
    "    q = mt(x)   \n",
    "\n",
    "for param in ms.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# train for mlp\n",
    "p_m = m(p)\n",
    "with torch.no_grad(): \n",
    "    q_m = m(q)\n",
    "\n",
    "for param in ms.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "p_md = m(p.detach())\n",
    "q_md = m(q.detach())\n",
    "\n",
    "ctr = kd_loss(p_md, q_md, 4) + kd_loss(p_m, q_m, 4)\n",
    "ctr.backward(retain_graph=True)\n",
    "ms.weight.grad, m.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Mean Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设有一个损失值张量\n",
    "losses = torch.tensor([0.0, 0.0, 0.0, 0.0])  # 这里只是举例，实际情况可能是一个批次的损失值\n",
    "\n",
    "# 计算损失的批次均值\n",
    "batch_mean = torch.mean(losses)\n",
    "print(\"Batch Mean Loss:\", batch_mean.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3086,  0.1316, -0.2693],\n",
      "        [ 0.0367, -0.0231,  0.0470],\n",
      "        [-0.3553,  0.1589, -0.3248],\n",
      "        [-0.3210,  0.1425, -0.2914],\n",
      "        [-0.2083,  0.0974, -0.1989]]) None\n",
      "tensor([[ 0.2311, -0.1373,  0.2795],\n",
      "        [-0.0049, -0.0013,  0.0024],\n",
      "        [ 0.0251,  0.0059, -0.0114],\n",
      "        [-0.0203,  0.0341, -0.0687],\n",
      "        [-0.2310,  0.0986, -0.2018]]) None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def kd_loss(y_s, y_t, temperature):\n",
    "    p_s = F.log_softmax(y_s/temperature, dim=1)\n",
    "    p_t = F.softmax(y_t/temperature, dim=1)\n",
    "    loss = nn.KLDivLoss(reduction='batchmean')(p_s, p_t) * (temperature**2)\n",
    "    return loss\n",
    "\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "gt = torch.ones((2), dtype=torch.long)\n",
    "\n",
    "ms = nn.Linear(3, 5)\n",
    "mt = nn.Linear(3, 5)\n",
    "\n",
    "m = nn.Linear(5, 4)\n",
    "\n",
    "opt = optim.SGD(nn.ModuleList([ms, m]).parameters(), lr=0.1)\n",
    "\n",
    "# way1\n",
    "opt.zero_grad()\n",
    "\n",
    "p = ms(x)  \n",
    "with torch.no_grad(): \n",
    "    q = mt(x)   \n",
    "\n",
    "for param in m.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# train for mlp\n",
    "p_m = m(p)\n",
    "with torch.no_grad(): \n",
    "    q_m = m(q)\n",
    "\n",
    "for param in m.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "p_md = m(p.detach())\n",
    "q_md = m(q.detach())\n",
    "\n",
    "ctr = kd_loss(p_m, q_m, 4) # kd_loss(p_md, q_md, 4) + \n",
    "ctr.backward(retain_graph=True)\n",
    "print(ms.weight.grad, m.weight.grad)\n",
    "\n",
    "opt.zero_grad()\n",
    "\n",
    "# way2\n",
    "p = ms(x)  \n",
    "with torch.no_grad(): \n",
    "    q = mt(x)   \n",
    "\n",
    "for param in m.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# train for mlp\n",
    "p_m = m(p)\n",
    "with torch.no_grad(): \n",
    "    q_m = m(q)\n",
    "\n",
    "for param in m.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "p_md = m(p.detach())\n",
    "q_md = m(q.detach())\n",
    "\n",
    "ctr = kd_loss(p, q, 4)\n",
    "ctr.backward(retain_graph=True)\n",
    "print(ms.weight.grad, m.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "a = torch.load(\"y_s_model.pt\")\n",
    "b = torch.load(\"y_t_model.pt\")\n",
    "\n",
    "plt.plot(a.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
